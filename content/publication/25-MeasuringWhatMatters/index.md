---
title: "Measuring what Matters: Construct Validity in Large Language Model Benchmarks"
authors:
- Andrew M. Bean
- Ryan Othniel Kearns
- Angelika Romanou
- Franziska Sofia Hafner
- Harry Mayne
- ...
- admin
- ...
- Adam Mahdi
date: "2025-010-30T00:00:00Z"
doi: ""

# Schedule page publish date (NOT publication's date).
publishDate: "2024-11-01T00:00:00Z"

# Publication type.
# Legend: 0 = Uncategorized; 1 = Conference paper; 2 = Journal article;
# 3 = Preprint / Working Paper; 4 = Report; 5 = Book; 6 = Book section;
# 7 = Thesis; 8 = Patent
publication_types: ["1"]

# Publication name and optional abbreviated publication name.
publication: "NeurIPS"
publication_short: "NeurIPS"

abstract: "Evaluating large language models (LLMs) is crucial for both assessing their capabilities and identifying safety or robustness issues prior to deployment. Reliably measuring abstract and complex phenomena such as safety and robustness requires strong construct validity, that is, having measures that represent what matters to the phenomenon. With a team of 29 expert reviewers, we conduct a systematic review of 445 LLM benchmarks from leading conferences in natural language processing and machine learning. Across the reviewed articles, we find patterns related to the measured phenomena, tasks, and scoring metrics which undermine the validity of the resulting claims. To address these shortcomings, we provide eight key recommendations and detailed actionable guidance to researchers and practitioners in developing LLM benchmarks."

# Summary. An optional shortened abstract.
summary: 'LLM benchmarks are essential for tracking progress and ensuring safety in AI, but most benchmarks don't measure what matters, as suggested by our systematic review of 445 LLM benchmarks from top AI conferences. A taxonomy of these failures was therefore built and translated into an operational checklist to help future benchmark authors demonstrate construct validity.'

#tags:
#- LLMs
#- NLP
#- Metacognition
featured: false

# Optional external URL for project (replaces project detail page).
external_link: 'https://oxrml.com/measuring-what-matters/'

links:
- name: NeurIPS Official Page
  url: https://neurips.cc/virtual/2025/loc/san-diego/poster/121477
- name: HF Repo
  url: https://huggingface.co/datasets/ambean/construct-validity-review
- name: Checklist
  url: https://oxrml.com/measuring-what-matters/checklist.html
url_pdf: 'https://openreview.net/pdf?id=mdA5lVvNcU'
url_code: 'https://github.com/yale-nlp/MetaFaith'
url_dataset: ''
url_poster: ''
url_project: 'https://oxrml.com/measuring-what-matters/'
url_slides: ''
url_source: ''
url_video: ''

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder. 
image:
  caption: ''
  focal_point: ""
  preview_only: false

# Associated Projects (optional).
#   Associate this publication with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `internal-project` references `content/project/internal-project/index.md`.
#   Otherwise, set `projects: []`.
#projects:
#- internal-project

# Slides (optional).
#   Associate this publication with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides: "example"` references `content/slides/example/index.md`.
#   Otherwise, set `slides: ""`.
slides: ""
---