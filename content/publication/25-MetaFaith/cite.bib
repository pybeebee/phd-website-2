@inproceedings{liu-etal-2025-metafaith,
    title = "{M}eta{F}aith: Faithful Natural Language Uncertainty Expression in {LLM}s",
    author = "Liu, Gabrielle Kaili-May  and
      Yona, Gal  and
      Caciularu, Avi  and
      Szpektor, Idan  and
      Rudner, Tim G. J.  and
      Cohan, Arman",
    editor = "Christodoulopoulos, Christos  and
      Chakraborty, Tanmoy  and
      Rose, Carolyn  and
      Peng, Violet",
    booktitle = "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2025",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.emnlp-main.1505/",
    doi = "10.18653/v1/2025.emnlp-main.1505",
    pages = "29600--29644",
    ISBN = "979-8-89176-332-6",
    abstract = "A critical component in the trustworthiness of LLMs is reliable uncertainty communication, yet LLMs often use assertive language when conveying false claims, leading to over-reliance and eroded trust. We present the first systematic study of {\_}faithful confidence calibration{\_} of LLMs, benchmarking models' ability to use linguistic expressions of uncertainty that {\_}faithfully reflect{\_} their intrinsic uncertainty, across a comprehensive array of models, datasets, and prompting strategies. Our results demonstrate that LLMs largely fail at this task, and that existing interventions are insufficient: standard prompt approaches provide only marginal gains, and existing, factuality-based calibration techniques can even harm faithful calibration. To address this critical gap, we introduce MetaFaith, a novel prompt-based calibration approach inspired by human metacognition. We show that MetaFaith robustly improves faithful calibration across diverse models and task domains, enabling up to 61{\%} improvement in faithfulness and achieving an 83{\%} win rate over original generations as judged by humans."
}